# Jobs
- company: University of Windsor &mdash; Windsor, ON
  position: Database Developer Coop
  duration: May 2024 &mdash; Aug 2024
  tech: Python (FastAPI, SQLite3, SQLAlchemy), React, Typescript, Docker, Digital Ocean, Github
  summary: <ul class="resume-item-list">
            <li>Developed a React-based website using TypeScript, State, Hooks, and Router, with a Python and PostgreSQL backend for efficient data handling and seamless integration.</li>
            <li>Designed the database schema and data ingestion scripts in Python operating on excel and CSV files.</li>
            <li>Utilized advanced Python features, including decorators, generators, context managers, and metaprogramming, to create efficient, reusable code and reduce boilerplate.</li>
            <li>Created a technical documentation, detailing usage instructions, application deployment, and code functionality.</li>
          </ul>

- company: University of Windsor &mdash; Windsor, ON
  position: Research Assistant
  duration: May 2023 &mdash; Apr 2024
  tech: Python (Tkinter, NumPy, Multithreading, Matplotlib, Scikit-Learn), FTP, VNC, Gitlab
  summary: <ul class="resume-item-list">
            <li>Worked with physicists to create a multi-threaded GUI app with real-time dynamic visualizations to stream video from a high-speed camera monitoring output of a laser experiment setup.</li>
            <li>Used techniques such as windowing and queues to efficiently distribute frames between multiple threads.</li>
            <li>Developed machine learning regression models to calibrate a spectrometer, achieving accurate results on a limited dataset.</li>
            <li>Learned and understood optics, fourier transforms, spatial coherency and usage of optical instruments.</li>
          </ul>

- company: Locobuzz Solutions Pvt. Ltd &mdash; Remote
  position: Python Development Consultant (Post-Employee Tenure)
  duration: June 2023 &mdash; May 2024
  tech: Python (FastAPI, Pandas, Pydantic), MS SQL, Clickhouse, Github
  summary: <ul class="resume-item-list">
            <li>Provided ongoing consulting services to my previous employer after 3 years of full-time work.</li>
            <li>Led the migration of the backend system from MSQ SQL to ClickHouse, improving database performance and scalability.</li>
            <li>Solved critical bugs identified during QA testing, leveraging my deep knowledge of the company's framework.</li>
            <li>Utilized extensive familiarity with the companyâ€™s codebase to ensure seamless integration and efficient troubleshooting.</li>
          </ul>

- company: Locobuzz Solutions Pvt. Ltd &mdash; Mumbai, India
  position: Data Engineer
  duration:  Jun 2021 &mdash; Apr 2023
  tech: Python (FastAPI, Pandas, Pydantic), Apache Airflow, CRON, MS SQL, ClickHouse, Redis, Docker, AWS, Azure, Jenkins, Github
  summary: <ul class="resume-item-list">
            <li>Created a data pipeline to simplify workflows, maximize readability and facilitate easy scaling of API endpoints. Leveraged appropriate caching techniques to boost API performance.</li>
            <li>Architected and developed a Python application that dynamically generates SQL queries based on contextual input.</li>
            <li>Engineered a scalable ETL pipeline in Pandas that utilized machine learning models to apply complex transformations on batch data, handling over 1 million records weekly.</li>
            <li>Implemented secure, high-performance async RESTful APIs using FastAPI, integrating Redis for caching, OAuth for authentication, and Pydantic for accurate data validation.</li>
            <li>Engineered an ETL pipeline in Apache AirFlow consuming AWS RedShift, MS SQL and Clickhouse for downstream analytics.</li>
          </ul>

- company: Locobuzz Solutions Pvt. Ltd &mdash; Mumbai, India
  position: Data Science Intern
  duration:  Jun 2020 &mdash; May 2021
  tech: Python (FastAPI, Pandas, Pydantic), Apache Airflow, CRON, MS SQL, ClickHouse, Redis, Docker, AWS, Azure, Jenkins, Github
  summary: <ul class="resume-item-list">
            <li>Created a data pipeline to simplify workflows, maximize readability and facilitate easy scaling of API endpoints. Leveraged appropriate caching techniques to boost API performance.</li>
            <li>Architected and developed a Python application that dynamically generates SQL queries based on contextual input.</li>
            <li>Engineered a scalable ETL pipeline in Pandas that utilized machine learning models to apply complex transformations on batch data, handling over 1 million records weekly.</li>
            <li>Implemented secure, high-performance async RESTful APIs using FastAPI, integrating Redis for caching, OAuth for authentication, and Pydantic for accurate data validation.</li>
            <li>Engineered an ETL pipeline in Apache AirFlow consuming AWS RedShift, MS SQL and Clickhouse for downstream analytics.</li>
          </ul>
